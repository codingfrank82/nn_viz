{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<ul> <li>TODO</li> </ul>"},{"location":"beginers/CNN/","title":"Convolutional Neural Networks","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.io import read_image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n</pre> import torch import torch.nn as nn from torchvision import transforms from torchvision.io import read_image  import numpy as np import matplotlib.pyplot as plt from tqdm.notebook import tqdm In\u00a0[2]: Copied! <pre>if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"No GPU detected, using CPU\")\n\ncpu_device = torch.device(\"cpu\")\n</pre> if torch.cuda.is_available():     device = torch.device(\"cuda:0\")     print(f\"GPU detected: {torch.cuda.get_device_name(0)}\") else:     device = torch.device(\"cpu\")     print(\"No GPU detected, using CPU\")  cpu_device = torch.device(\"cpu\") <pre>GPU detected: NVIDIA T400 4GB\n</pre> In\u00a0[3]: Copied! <pre>def show_image(images, labels):\n    fix, axs = plt.subplots(ncols=len(images), squeeze=False, figsize=(12, 6))\n    for i, (image, label) in enumerate(zip(images, labels)):\n        img = transforms.ToPILImage()(image.to('cpu'))\n        axs[0, i].imshow(np.asarray(img), cmap='gray')\n        axs[0, i].set_title(label)\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n</pre> def show_image(images, labels):     fix, axs = plt.subplots(ncols=len(images), squeeze=False, figsize=(12, 6))     for i, (image, label) in enumerate(zip(images, labels)):         img = transforms.ToPILImage()(image.to('cpu'))         axs[0, i].imshow(np.asarray(img), cmap='gray')         axs[0, i].set_title(label)         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]) In\u00a0[4]: Copied! <pre>image_transforms = transforms.Compose([\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Grayscale()\n])\n</pre> image_transforms = transforms.Compose([     transforms.ConvertImageDtype(torch.float),     transforms.Grayscale() ]) In\u00a0[5]: Copied! <pre>origin_img = read_image('../../assets/images/lena_std.png')\ntrans_img = image_transforms(origin_img)\nshow_image([origin_img, trans_img], ['Original Image', 'Transformed Image'])\n</pre> origin_img = read_image('../../assets/images/lena_std.png') trans_img = image_transforms(origin_img) show_image([origin_img, trans_img], ['Original Image', 'Transformed Image']) In\u00a0[6]: Copied! <pre>class SobelEdgeDetection(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_x = nn.Conv2d(1, 1, padding=1, kernel_size=3, bias=False)\n        self.conv_y = nn.Conv2d(1, 1, padding=1, kernel_size=3, bias=False)\n\n    def forward(self, x):\n        g_x = torch.abs(self.conv_x(x))\n        g_y = torch.abs(self.conv_y(x))\n        return g_x+g_y\n\nedge_detector = SobelEdgeDetection()\nprint(edge_detector)\nprint(edge_detector.conv_x.weight.shape)\n</pre> class SobelEdgeDetection(nn.Module):     def __init__(self):         super().__init__()         self.conv_x = nn.Conv2d(1, 1, padding=1, kernel_size=3, bias=False)         self.conv_y = nn.Conv2d(1, 1, padding=1, kernel_size=3, bias=False)      def forward(self, x):         g_x = torch.abs(self.conv_x(x))         g_y = torch.abs(self.conv_y(x))         return g_x+g_y  edge_detector = SobelEdgeDetection() print(edge_detector) print(edge_detector.conv_x.weight.shape) <pre>SobelEdgeDetection(\n  (conv_x): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv_y): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n)\ntorch.Size([1, 1, 3, 3])\n</pre> In\u00a0[7]: Copied! <pre>sobel_x = torch.tensor([\n    [-1, 0, 1],\n    [-2, 0, 2],\n    [-1, 0, 1]],\n    dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nsobel_y = torch.tensor([\n    [-1, -2, -1],\n    [0, 0, 0],\n    [1, 2, 1]],\n    dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n\nedge_detector.conv_x.weight.data = sobel_x\nedge_detector.conv_x.weight.requires_grad = False\nedge_detector.conv_y.weight.data = sobel_y\nedge_detector.conv_y.weight.requires_grad = False\n</pre> sobel_x = torch.tensor([     [-1, 0, 1],     [-2, 0, 2],     [-1, 0, 1]],     dtype=torch.float32).unsqueeze(0).unsqueeze(0) sobel_y = torch.tensor([     [-1, -2, -1],     [0, 0, 0],     [1, 2, 1]],     dtype=torch.float32).unsqueeze(0).unsqueeze(0)  edge_detector.conv_x.weight.data = sobel_x edge_detector.conv_x.weight.requires_grad = False edge_detector.conv_y.weight.data = sobel_y edge_detector.conv_y.weight.requires_grad = False In\u00a0[8]: Copied! <pre>filter_image = edge_detector(trans_img)\n\nprint(filter_image.shape)\n</pre> filter_image = edge_detector(trans_img)  print(filter_image.shape) <pre>torch.Size([1, 512, 512])\n</pre> In\u00a0[9]: Copied! <pre>show_image([origin_img, filter_image], ['Original Image', 'Edge Detection'])\n</pre> show_image([origin_img, filter_image], ['Original Image', 'Edge Detection']) In\u00a0[10]: Copied! <pre>trans_img = trans_img.to(device)\nfilter_image = filter_image.to(device)\nprint(filter_image.device)\n</pre> trans_img = trans_img.to(device) filter_image = filter_image.to(device) print(filter_image.device) <pre>cuda:0\n</pre> In\u00a0[11]: Copied! <pre>learning_rate = 0.1\nnum_epochs = 10000\n\nmodel = SobelEdgeDetection()\nmodel.train()\nmodel.to(device)\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nfor _ in tqdm(range(num_epochs)):\n    optimizer.zero_grad()\n    y_pred = model(trans_img)\n\n    loss = loss_fn(y_pred, filter_image)\n    loss.backward()\n    optimizer.step()\n</pre> learning_rate = 0.1 num_epochs = 10000  model = SobelEdgeDetection() model.train() model.to(device)  loss_fn = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  for _ in tqdm(range(num_epochs)):     optimizer.zero_grad()     y_pred = model(trans_img)      loss = loss_fn(y_pred, filter_image)     loss.backward()     optimizer.step() <pre>  0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre>show_image([origin_img, y_pred], ['Original Image', 'Pred Image'])\nprint('pred soble x: ', model.conv_x.weight)\nprint('pred soble y: ', model.conv_y.weight)\n</pre> show_image([origin_img, y_pred], ['Original Image', 'Pred Image']) print('pred soble x: ', model.conv_x.weight) print('pred soble y: ', model.conv_y.weight) <pre>pred soble x:  Parameter containing:\ntensor([[[[ 1.6291,  0.3754, -0.6821],\n          [ 1.8895,  0.1116, -1.6627],\n          [ 0.6297, -0.5741, -1.7144]]]], device='cuda:0', requires_grad=True)\npred soble y:  Parameter containing:\ntensor([[[[ 8.2955e-01,  1.5082e+00,  9.9381e-01],\n          [-7.1158e-01, -1.1744e-03,  1.3052e-01],\n          [-7.4955e-01, -1.3533e+00, -6.4669e-01]]]], device='cuda:0',\n       requires_grad=True)\n</pre> In\u00a0[1]: Copied! <pre># !pip install scikit-image\n</pre> # !pip install scikit-image In\u00a0[14]: Copied! <pre>pred_image = y_pred.detach().cpu().numpy()\ntrue_image = filter_image.detach().cpu().numpy()\nprint(pred_image.shape)\n</pre> pred_image = y_pred.detach().cpu().numpy() true_image = filter_image.detach().cpu().numpy() print(pred_image.shape) <pre>(1, 512, 512)\n</pre> In\u00a0[15]: Copied! <pre>from skimage.metrics import structural_similarity as ssim\n\n# Compute the Mean Squared Error (MSE)\nmse = np.mean((pred_image - true_image) ** 2)\nprint(f'Mean Squared Error (MSE): {mse:.4f}')\n\n# Compute the Structural Similarity Index Measure (SSIM)\nssim_value = ssim(pred_image, true_image, multichannel=True, channel_axis=0, data_range=1)\nprint(f'Structural Similarity Index Measure (SSIM): {ssim_value}')\n</pre> from skimage.metrics import structural_similarity as ssim  # Compute the Mean Squared Error (MSE) mse = np.mean((pred_image - true_image) ** 2) print(f'Mean Squared Error (MSE): {mse:.4f}')  # Compute the Structural Similarity Index Measure (SSIM) ssim_value = ssim(pred_image, true_image, multichannel=True, channel_axis=0, data_range=1) print(f'Structural Similarity Index Measure (SSIM): {ssim_value}') <pre>Mean Squared Error (MSE): 0.0032\nStructural Similarity Index Measure (SSIM): 0.9611660242080688\n</pre> In\u00a0[15]: Copied! <pre>\n</pre>"},{"location":"beginers/CNN/#convolutional-neural-networks","title":"Convolutional Neural Networks\u00b6","text":""},{"location":"beginers/CNN/#load-image","title":"Load Image\u00b6","text":""},{"location":"beginers/CNN/#convolutions-for-edge-detection","title":"Convolutions for Edge Detection\u00b6","text":""},{"location":"beginers/CNN/#sobel-edge-detector","title":"Sobel Edge Detector\u00b6","text":""},{"location":"beginers/CNN/#learning-a-kernel","title":"Learning a Kernel\u00b6","text":""},{"location":"beginers/CNN/#show-result","title":"Show Result\u00b6","text":""},{"location":"beginers/CNN/#error-analysis","title":"Error analysis\u00b6","text":""},{"location":"beginers/CNN/#references","title":"References\u00b6","text":"<ul> <li> A guide to convolution arithmetic for deep learning </li> <li> Sobel Edge Detector </li> </ul>"},{"location":"beginers/MLP/","title":"Multilayer Perceptrons","text":"In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom torchsummary import summary\n</pre> import torch import torch.nn as nn import torch.nn.init as init import matplotlib.pyplot as plt from tqdm.notebook import tqdm from torchsummary import summary In\u00a0[2]: Copied! <pre>if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"No GPU detected, using CPU\")\n\ncpu_device = torch.device(\"cpu\")\n</pre> if torch.cuda.is_available():     device = torch.device(\"cuda:0\")     print(f\"GPU detected: {torch.cuda.get_device_name(0)}\") else:     device = torch.device(\"cpu\")     print(\"No GPU detected, using CPU\")  cpu_device = torch.device(\"cpu\") <pre>GPU detected: NVIDIA T400 4GB\n</pre> In\u00a0[3]: Copied! <pre>def func_c(x):\n    return torch.sin(x ** 2 + 3) + torch.cos(2 * x ** 3 + 3 * x ** 2 + 2)\n\n\ndef func_b(x):\n    return torch.sin(x ** 2 + 3)\n\n\ndef func_a(x):\n    return 5 * x ** 2 + 3\n\nfunc_range = (-1, 1)\nobjective_func = lambda x, device: func_c(x) + torch.normal(0, 0.01, x.shape).to(device)\n</pre> def func_c(x):     return torch.sin(x ** 2 + 3) + torch.cos(2 * x ** 3 + 3 * x ** 2 + 2)   def func_b(x):     return torch.sin(x ** 2 + 3)   def func_a(x):     return 5 * x ** 2 + 3  func_range = (-1, 1) objective_func = lambda x, device: func_c(x) + torch.normal(0, 0.01, x.shape).to(device) In\u00a0[4]: Copied! <pre>def my_train(model, input_size, target_func, func_range, device, learning_rate=0.001):\n    loss_fn = nn.MSELoss()\n    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    num_epochs = 10000\n    batch_size = 128\n\n    model.train()\n    model.to(device)\n    a = func_range[0]\n    b = func_range[1]\n\n    for epoch in tqdm(range(num_epochs), dynamic_ncols=True, desc=f'Training on {device}'):\n        optimizer.zero_grad()\n\n        xx = a + (b - a) * torch.rand(batch_size, input_size)\n        xx = xx.to(device)\n        y_pred = model(xx)\n        y = objective_func(xx, device)\n        loss = loss_fn(y_pred, y)\n\n        loss.backward()\n        optimizer.step()\n    print(f\"Loss value: {loss.item()}\")\n</pre> def my_train(model, input_size, target_func, func_range, device, learning_rate=0.001):     loss_fn = nn.MSELoss()     # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)      num_epochs = 10000     batch_size = 128      model.train()     model.to(device)     a = func_range[0]     b = func_range[1]      for epoch in tqdm(range(num_epochs), dynamic_ncols=True, desc=f'Training on {device}'):         optimizer.zero_grad()          xx = a + (b - a) * torch.rand(batch_size, input_size)         xx = xx.to(device)         y_pred = model(xx)         y = objective_func(xx, device)         loss = loss_fn(y_pred, y)          loss.backward()         optimizer.step()     print(f\"Loss value: {loss.item()}\") In\u00a0[5]: Copied! <pre>def xavier_init(model):\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear):\n            init.xavier_normal_(layer.weight)\n</pre> def xavier_init(model):     for layer in model.modules():         if isinstance(layer, nn.Linear):             init.xavier_normal_(layer.weight) In\u00a0[6]: Copied! <pre>input_size = 1\nhidden_size = 1024\noutput_size = 1\n</pre> input_size = 1 hidden_size = 1024 output_size = 1 In\u00a0[7]: Copied! <pre>shallow_network = torch.nn.Sequential(\n    nn.Linear(input_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, output_size)\n)\nshallow_network.to(device)\nshallow_network.apply(xavier_init)\n\nsummary(shallow_network, input_size=(1,))\n</pre> shallow_network = torch.nn.Sequential(     nn.Linear(input_size, hidden_size),     nn.ReLU(),     nn.Linear(hidden_size, output_size) ) shallow_network.to(device) shallow_network.apply(xavier_init)  summary(shallow_network, input_size=(1,)) <pre>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                 [-1, 1024]           2,048\n              ReLU-2                 [-1, 1024]               0\n            Linear-3                    [-1, 1]           1,025\n================================================================\nTotal params: 3,073\nTrainable params: 3,073\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.03\n----------------------------------------------------------------\n</pre> In\u00a0[8]: Copied! <pre>my_train(shallow_network, input_size, objective_func, func_range, device)\nprint('done.')\n</pre> my_train(shallow_network, input_size, objective_func, func_range, device) print('done.') <pre>Training on cuda:0:   0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Loss value: 0.007621679455041885\ndone.\n</pre> In\u00a0[9]: Copied! <pre>def plot_function_fit(obj_func, models, func_range, device):\n\"\"\"\n    plot the function fitting result\n    :param obj_func: Objection Function\n    :param models: NN Model\n    :param func_range: function range\n    :param device: GPU or CPU\n    \"\"\"\n\n    data_x = torch.linspace(func_range[0], func_range[1], 100).unsqueeze(1)\n    data_y = obj_func(data_x, cpu_device)\n\n    pred_x = torch.linspace(func_range[0], func_range[1], 200).unsqueeze(1)\n\n    plt.figure(figsize=(8, 8))\n    plt.scatter(data_x, data_y, label=\"Data\", c='yellow')\n\n    for model in models:\n        pred_y = model(pred_x.to(device))\n        plt.plot(pred_x, pred_y.detach().cpu().numpy(), label=type(model).__name__)\n\n    plt.grid()\n    plt.legend()\n    plt.show()\n</pre> def plot_function_fit(obj_func, models, func_range, device):     \"\"\"     plot the function fitting result     :param obj_func: Objection Function     :param models: NN Model     :param func_range: function range     :param device: GPU or CPU     \"\"\"      data_x = torch.linspace(func_range[0], func_range[1], 100).unsqueeze(1)     data_y = obj_func(data_x, cpu_device)      pred_x = torch.linspace(func_range[0], func_range[1], 200).unsqueeze(1)      plt.figure(figsize=(8, 8))     plt.scatter(data_x, data_y, label=\"Data\", c='yellow')      for model in models:         pred_y = model(pred_x.to(device))         plt.plot(pred_x, pred_y.detach().cpu().numpy(), label=type(model).__name__)      plt.grid()     plt.legend()     plt.show()  In\u00a0[10]: Copied! <pre>shallow_network.eval()\nplot_function_fit(objective_func, [shallow_network], func_range, device)\n</pre> shallow_network.eval() plot_function_fit(objective_func, [shallow_network], func_range, device) In\u00a0[11]: Copied! <pre>class MyMLP(nn.Module):\n    def __init__(self, input_size, hidden_layers, hidden_size, output_size):\n        super(MyMLP, self).__init__()\n\n        # Input layer\n        self.layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n\n        # Hidden layers\n        for _ in range(hidden_layers - 1):\n            self.layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n\n        # Output layer\n        self.layers.append(nn.Linear(hidden_size, output_size))\n\n        self.model = nn.Sequential(*self.layers)\n\n    def forward(self, x):\n        return self.model(x)\n</pre> class MyMLP(nn.Module):     def __init__(self, input_size, hidden_layers, hidden_size, output_size):         super(MyMLP, self).__init__()          # Input layer         self.layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]          # Hidden layers         for _ in range(hidden_layers - 1):             self.layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])          # Output layer         self.layers.append(nn.Linear(hidden_size, output_size))          self.model = nn.Sequential(*self.layers)      def forward(self, x):         return self.model(x)  In\u00a0[12]: Copied! <pre>mlp = MyMLP(input_size, 8, 16, output_size)\nmlp.apply(xavier_init)\nmlp.to(device)\n\nsummary(mlp, input_size=(1,))\n</pre> mlp = MyMLP(input_size, 8, 16, output_size) mlp.apply(xavier_init) mlp.to(device)  summary(mlp, input_size=(1,)) <pre>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                   [-1, 16]              32\n              ReLU-2                   [-1, 16]               0\n            Linear-3                   [-1, 16]             272\n              ReLU-4                   [-1, 16]               0\n            Linear-5                   [-1, 16]             272\n              ReLU-6                   [-1, 16]               0\n            Linear-7                   [-1, 16]             272\n              ReLU-8                   [-1, 16]               0\n            Linear-9                   [-1, 16]             272\n             ReLU-10                   [-1, 16]               0\n           Linear-11                   [-1, 16]             272\n             ReLU-12                   [-1, 16]               0\n           Linear-13                   [-1, 16]             272\n             ReLU-14                   [-1, 16]               0\n           Linear-15                   [-1, 16]             272\n             ReLU-16                   [-1, 16]               0\n           Linear-17                    [-1, 1]              17\n================================================================\nTotal params: 1,953\nTrainable params: 1,953\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.01\n----------------------------------------------------------------\n</pre> In\u00a0[13]: Copied! <pre>my_train(mlp, input_size, objective_func, func_range, device)\n</pre> my_train(mlp, input_size, objective_func, func_range, device) <pre>Training on cuda:0:   0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Loss value: 0.00036084422026760876\n</pre> In\u00a0[14]: Copied! <pre>mlp.eval()\nplot_function_fit(objective_func, [mlp, shallow_network], func_range, device)\n</pre> mlp.eval() plot_function_fit(objective_func, [mlp, shallow_network], func_range, device) In\u00a0[15]: Copied! <pre>input_size = 16\noutput_size = 16\n\nfunc_range_m = [\n    torch.ones(input_size)*-1,\n    torch.ones(input_size)\n]\n</pre> input_size = 16 output_size = 16  func_range_m = [     torch.ones(input_size)*-1,     torch.ones(input_size) ]  In\u00a0[16]: Copied! <pre>mlp = MyMLP(input_size, 8, 256, output_size)\nmlp.apply(xavier_init)\nmy_train(mlp, input_size, objective_func, func_range_m, device, 1e-4)\n</pre> mlp = MyMLP(input_size, 8, 256, output_size) mlp.apply(xavier_init) my_train(mlp, input_size, objective_func, func_range_m, device, 1e-4) <pre>Training on cuda:0:   0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Loss value: 0.009449731558561325\n</pre> In\u00a0[17]: Copied! <pre>shallow_network = torch.nn.Sequential(\n    nn.Linear(input_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, output_size)\n)\nshallow_network.apply(xavier_init)\n\nmy_train(shallow_network, input_size, objective_func, func_range, device)\n</pre> shallow_network = torch.nn.Sequential(     nn.Linear(input_size, hidden_size),     nn.ReLU(),     nn.Linear(hidden_size, output_size) ) shallow_network.apply(xavier_init)  my_train(shallow_network, input_size, objective_func, func_range, device) <pre>Training on cuda:0:   0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Loss value: 0.004353749565780163\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"beginers/MLP/#mlp-as-universal-approximator","title":"MLP as Universal Approximator\u00b6","text":"<ul> <li>Universal Approximation theorem<ul> <li>A multi-layer perceptron can be used to approximate any function.</li> </ul> </li> </ul>"},{"location":"beginers/MLP/#activation-functions","title":"Activation Functions\u00b6","text":""},{"location":"beginers/MLP/#sigmoid","title":"Sigmoid\u00b6","text":"<p>$$ \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$</p>"},{"location":"beginers/MLP/#relu","title":"ReLU\u00b6","text":"<ul> <li>The Rectified Linear Unit function</li> </ul> <p>$$ \\text{ReLU}(x) = (x)^+ = \\max(0, x) $$</p>"},{"location":"beginers/MLP/#tanh","title":"Tanh\u00b6","text":"<ul> <li>Hyperbolic Tangent</li> </ul> <p>$$ \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)} $$</p>"},{"location":"beginers/MLP/#experiments","title":"Experiments\u00b6","text":""},{"location":"beginers/MLP/#objective-functions","title":"Objective Functions\u00b6","text":""},{"location":"beginers/MLP/#function-fitting","title":"Function Fitting\u00b6","text":""},{"location":"beginers/MLP/#shallow-network","title":"Shallow Network\u00b6","text":""},{"location":"beginers/MLP/#visualization","title":"Visualization\u00b6","text":""},{"location":"beginers/MLP/#mlp","title":"MLP\u00b6","text":""},{"location":"beginers/MLP/#high-dimension-functions","title":"High Dimension Functions\u00b6","text":""},{"location":"experiments/FourierFeatures/","title":"Fourier Features","text":"In\u00a0[1]: Copied! <pre>import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n</pre> import os from PIL import Image import numpy as np import torch import torch.nn as nn import torch.nn.init as init import torchvision.transforms as transforms import matplotlib.pyplot as plt from tqdm.notebook import tqdm In\u00a0[2]: Copied! <pre>def try_gpu(i=0):\n\"\"\" Return gpu(i) if exists, otherwise return cpu(). \"\"\"\n    if torch.cuda.is_available():\n        print(f\"GPU detected: {torch.cuda.get_device_name(i)}\")\n        return torch.device(f'cuda:{i}')\n    else:\n        print(\"No GPU detected, using CPU\")\n        return torch.device('cpu')\n</pre> def try_gpu(i=0):     \"\"\" Return gpu(i) if exists, otherwise return cpu(). \"\"\"     if torch.cuda.is_available():         print(f\"GPU detected: {torch.cuda.get_device_name(i)}\")         return torch.device(f'cuda:{i}')     else:         print(\"No GPU detected, using CPU\")         return torch.device('cpu') In\u00a0[3]: Copied! <pre>device = try_gpu()\nprint('Using device:', device)\n</pre> device = try_gpu() print('Using device:', device) <pre>GPU detected: NVIDIA T400 4GB\nUsing device: cuda:0\n</pre> In\u00a0[4]: Copied! <pre>class MyMLP(nn.Module):\n    def __init__(self, input_size, hidden_layers, hidden_size, output_size, input_mapping_layer):\n        super(MyMLP, self).__init__()\n\n        # Input layer\n        if input_mapping_layer != None:\n            input_mapping_size = input_mapping_layer.get_output_size()\n            self.name = 'MLP with ' + input_mapping_layer.get_name()\n            self.layers = [input_mapping_layer, nn.Linear(input_mapping_size, hidden_size), nn.ReLU()]\n        else:\n            self.name = 'Standard MLP'\n            self.layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n\n        # Hidden layers\n        for _ in range(hidden_layers - 1):\n            self.layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n\n        # Output layer\n        self.layers.append(nn.Linear(hidden_size, output_size))\n\n        self.model = nn.Sequential(*self.layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def get_name(self):\n        return self.name\n</pre> class MyMLP(nn.Module):     def __init__(self, input_size, hidden_layers, hidden_size, output_size, input_mapping_layer):         super(MyMLP, self).__init__()          # Input layer         if input_mapping_layer != None:             input_mapping_size = input_mapping_layer.get_output_size()             self.name = 'MLP with ' + input_mapping_layer.get_name()             self.layers = [input_mapping_layer, nn.Linear(input_mapping_size, hidden_size), nn.ReLU()]         else:             self.name = 'Standard MLP'             self.layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]          # Hidden layers         for _ in range(hidden_layers - 1):             self.layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])          # Output layer         self.layers.append(nn.Linear(hidden_size, output_size))          self.model = nn.Sequential(*self.layers)      def forward(self, x):         return self.model(x)      def get_name(self):         return self.name  In\u00a0[5]: Copied! <pre>image_path = '../../assets/images/fox.jpg'\nimage = Image.open(image_path)\nmin_side = min(image.size) - 32\n\ntransform = transforms.Compose([\n    transforms.CenterCrop(min_side),\n    transforms.Resize(128),\n    transforms.ToTensor()\n])\n\ntensor_image = transform(image).permute(1, 2, 0)\n</pre> image_path = '../../assets/images/fox.jpg' image = Image.open(image_path) min_side = min(image.size) - 32  transform = transforms.Compose([     transforms.CenterCrop(min_side),     transforms.Resize(128),     transforms.ToTensor() ])  tensor_image = transform(image).permute(1, 2, 0) In\u00a0[6]: Copied! <pre>plt.figure(figsize=(8, 8))\nplt.imshow(tensor_image)\nplt.show()\n</pre> plt.figure(figsize=(8, 8)) plt.imshow(tensor_image) plt.show() In\u00a0[7]: Copied! <pre>tensor_image = tensor_image.to(device)\n</pre> tensor_image = tensor_image.to(device) In\u00a0[8]: Copied! <pre>def get_line_of_rgb_values(img, y):\n    return img[:, y]\n\n\ndef get_line_of_uv(y, width, height):\n    u = torch.arange(0, width, dtype=torch.float32) / width\n    v = torch.zeros(width, dtype=torch.float32).fill_(y / height)\n    return torch.stack((u, v), dim=1).to(device)\n</pre> def get_line_of_rgb_values(img, y):     return img[:, y]   def get_line_of_uv(y, width, height):     u = torch.arange(0, width, dtype=torch.float32) / width     v = torch.zeros(width, dtype=torch.float32).fill_(y / height)     return torch.stack((u, v), dim=1).to(device)  In\u00a0[10]: Copied! <pre>def my_image_train(model, image, learning_rate, num_epochs):\n    image_width = image.shape[0]\n    image_height = image.shape[1]\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    model.train()\n    model.to(device)\n\n    for _ in tqdm(range(num_epochs), dynamic_ncols=True, desc=f'Training on {device}'):\n        optimizer.zero_grad()\n\n        for y in range(image_height):\n            uv = get_line_of_uv(y, image_width, image_height)\n            rgb = get_line_of_rgb_values(image, y)\n            rgb_pred = model(uv)\n\n            loss = loss_fn(rgb_pred, rgb)\n            loss.backward()\n            optimizer.step()\n</pre> def my_image_train(model, image, learning_rate, num_epochs):     image_width = image.shape[0]     image_height = image.shape[1]      loss_fn = nn.MSELoss()     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)      model.train()     model.to(device)      for _ in tqdm(range(num_epochs), dynamic_ncols=True, desc=f'Training on {device}'):         optimizer.zero_grad()          for y in range(image_height):             uv = get_line_of_uv(y, image_width, image_height)             rgb = get_line_of_rgb_values(image, y)             rgb_pred = model(uv)              loss = loss_fn(rgb_pred, rgb)             loss.backward()             optimizer.step() In\u00a0[11]: Copied! <pre>def xavier_init(model):\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear):\n            init.xavier_normal_(layer.weight)\n</pre> def xavier_init(model):     for layer in model.modules():         if isinstance(layer, nn.Linear):             init.xavier_normal_(layer.weight) In\u00a0[12]: Copied! <pre>class BasicMappingLayer(nn.Module):\n    def __init__(self, input_size):\n        super(BasicMappingLayer, self).__init__()\n        self.input_size = input_size\n\n    def forward(self, v):\n        v_proj = 2.0 * torch.pi * v\n        return torch.cat([torch.cos(v_proj), torch.sin(v_proj)], dim=-1)\n\n    def get_output_size(self):\n        return self.input_size * 2\n\n    def get_name(self):\n        return 'basic mapping'\n</pre> class BasicMappingLayer(nn.Module):     def __init__(self, input_size):         super(BasicMappingLayer, self).__init__()         self.input_size = input_size      def forward(self, v):         v_proj = 2.0 * torch.pi * v         return torch.cat([torch.cos(v_proj), torch.sin(v_proj)], dim=-1)      def get_output_size(self):         return self.input_size * 2      def get_name(self):         return 'basic mapping' In\u00a0[13]: Copied! <pre>class PositionalEncodingLayer(nn.Module):\n    def __init__(self, input_size, mapping_size, scale):\n        super(PositionalEncodingLayer, self).__init__()\n        self.input_size = input_size\n        self.mapping_size = mapping_size\n        self.scale = scale\n\n    def forward(self, x):\n        rets = []\n        m = self.mapping_size - 1\n        for j in range(m):\n            for fn in [torch.sin, torch.cos]:\n                s = self.scale ** (j / m)\n                rets.append(fn(2.0 * torch.pi * s * x))\n        return torch.cat(rets, dim=-1)\n\n    def get_output_size(self):\n        m = self.mapping_size - 1\n        return self.input_size * 2 * m\n\n    def get_name(self):\n        return 'positional encoding'\n</pre>  class PositionalEncodingLayer(nn.Module):     def __init__(self, input_size, mapping_size, scale):         super(PositionalEncodingLayer, self).__init__()         self.input_size = input_size         self.mapping_size = mapping_size         self.scale = scale      def forward(self, x):         rets = []         m = self.mapping_size - 1         for j in range(m):             for fn in [torch.sin, torch.cos]:                 s = self.scale ** (j / m)                 rets.append(fn(2.0 * torch.pi * s * x))         return torch.cat(rets, dim=-1)      def get_output_size(self):         m = self.mapping_size - 1         return self.input_size * 2 * m      def get_name(self):         return 'positional encoding' In\u00a0[14]: Copied! <pre>class GaussianFourierMappingLayer(nn.Module):\n    def __init__(self, input_size, mapping_size, scale):\n        super(GaussianFourierMappingLayer, self).__init__()\n        self.input_size = input_size\n        self.mapping_size = mapping_size\n        self.B = nn.Parameter(torch.normal(mean=0, std=scale, size=(mapping_size, input_size)), requires_grad=True)\n        self.scale = scale\n\n    def forward(self, x):\n        x_proj = (2.0 * torch.pi * x) @ self.B.T\n        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n\n    def get_output_size(self):\n        return self.input_size * self.mapping_size\n\n    def get_name(self):\n        return f'Gaussian {self.scale}'\n</pre> class GaussianFourierMappingLayer(nn.Module):     def __init__(self, input_size, mapping_size, scale):         super(GaussianFourierMappingLayer, self).__init__()         self.input_size = input_size         self.mapping_size = mapping_size         self.B = nn.Parameter(torch.normal(mean=0, std=scale, size=(mapping_size, input_size)), requires_grad=True)         self.scale = scale      def forward(self, x):         x_proj = (2.0 * torch.pi * x) @ self.B.T         return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)      def get_output_size(self):         return self.input_size * self.mapping_size      def get_name(self):         return f'Gaussian {self.scale}' In\u00a0[15]: Copied! <pre>input_size = 2\nhidden_layers = 8\nhidden_size = 256\noutput_size = 3\n\nmapping_size = 16\nscale = 10\n\n# Standard MLP\nmodels = [MyMLP(input_size, hidden_layers, hidden_size, output_size, None)]\n\n# Basic mapping\nbasic_input_mapping = BasicMappingLayer(input_size)\nmodels.append(\n    MyMLP(input_size, hidden_layers, hidden_size, output_size, basic_input_mapping)\n)\n\n# Positional encoding\npositional_encoding = PositionalEncodingLayer(input_size, mapping_size, scale)\nmodels.append(\n    MyMLP(input_size, hidden_layers, hidden_size, output_size, positional_encoding)\n)\n\n# Fourier mapping\nfourier_mapping = GaussianFourierMappingLayer(input_size, mapping_size, scale)\nmodels.append(\n    MyMLP(input_size, hidden_layers, hidden_size, output_size, fourier_mapping)\n)\n\nfourier_mapping = GaussianFourierMappingLayer(input_size, mapping_size, scale ** 2)\nmodels.append(\n    MyMLP(input_size, hidden_layers, hidden_size, output_size, fourier_mapping)\n)\n\nprint(f'image size: ({tensor_image.shape[0]}, {tensor_image.shape[1]})')\nfor model in models:\n    state_dict_file = f'state_dict/{model.get_name()}.pt'\n    if os.path.isfile(state_dict_file):\n        model.load_state_dict(torch.load(state_dict_file))\n        model.to(device)\n        print(\"Loaded state_dict from\", state_dict_file)\n    else:\n        print('Model: ', model.get_name())\n        model.apply(xavier_init)\n        my_image_train(model, tensor_image, 1e-4, 2000)\n        torch.save(model.state_dict(), state_dict_file)\n</pre> input_size = 2 hidden_layers = 8 hidden_size = 256 output_size = 3  mapping_size = 16 scale = 10  # Standard MLP models = [MyMLP(input_size, hidden_layers, hidden_size, output_size, None)]  # Basic mapping basic_input_mapping = BasicMappingLayer(input_size) models.append(     MyMLP(input_size, hidden_layers, hidden_size, output_size, basic_input_mapping) )  # Positional encoding positional_encoding = PositionalEncodingLayer(input_size, mapping_size, scale) models.append(     MyMLP(input_size, hidden_layers, hidden_size, output_size, positional_encoding) )  # Fourier mapping fourier_mapping = GaussianFourierMappingLayer(input_size, mapping_size, scale) models.append(     MyMLP(input_size, hidden_layers, hidden_size, output_size, fourier_mapping) )  fourier_mapping = GaussianFourierMappingLayer(input_size, mapping_size, scale ** 2) models.append(     MyMLP(input_size, hidden_layers, hidden_size, output_size, fourier_mapping) )  print(f'image size: ({tensor_image.shape[0]}, {tensor_image.shape[1]})') for model in models:     state_dict_file = f'state_dict/{model.get_name()}.pt'     if os.path.isfile(state_dict_file):         model.load_state_dict(torch.load(state_dict_file))         model.to(device)         print(\"Loaded state_dict from\", state_dict_file)     else:         print('Model: ', model.get_name())         model.apply(xavier_init)         my_image_train(model, tensor_image, 1e-4, 2000)         torch.save(model.state_dict(), state_dict_file) <pre>image size: (128, 128)\nLoaded state_dict from state_dict/Standard MLP.pt\nLoaded state_dict from state_dict/MLP with basic mapping.pt\nLoaded state_dict from state_dict/MLP with positional encoding.pt\nModel:  MLP with Gaussian 10\n</pre> <pre>Training on cuda:0:   0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <pre>Model:  MLP with Gaussian 100\n</pre> <pre>Training on cuda:0:   0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>image_width = tensor_image.shape[0]\nimage_height = tensor_image.shape[1]\nmodel_count = len(models)\n\nnrows = 2\nncols = int(np.ceil((model_count + 1) / 2))\n\nplt.figure(figsize=(8 * ncols, 8 * nrows))\nplt.subplot(nrows, ncols, 1)\nplt.title('GT')\nplt.imshow(tensor_image.detach().cpu().numpy())\n\nfor i in range(model_count):\n    model = models[i]\n    model.eval()\n    image_pred = torch.zeros_like(tensor_image)\n    for y in range(image_height):\n        uv = get_line_of_uv(y, image_width, image_height)\n        rgb_pred = model(uv)\n        image_pred[:, y] = rgb_pred\n\n    plt.subplot(nrows, ncols, i + 2)\n    plt.imshow(image_pred.detach().cpu().numpy())\n    plt.title(model.get_name())\n\nplt.savefig('plot.png', dpi=300, bbox_inches='tight', transparent=True)\nplt.show()\n</pre> image_width = tensor_image.shape[0] image_height = tensor_image.shape[1] model_count = len(models)  nrows = 2 ncols = int(np.ceil((model_count + 1) / 2))  plt.figure(figsize=(8 * ncols, 8 * nrows)) plt.subplot(nrows, ncols, 1) plt.title('GT') plt.imshow(tensor_image.detach().cpu().numpy())  for i in range(model_count):     model = models[i]     model.eval()     image_pred = torch.zeros_like(tensor_image)     for y in range(image_height):         uv = get_line_of_uv(y, image_width, image_height)         rgb_pred = model(uv)         image_pred[:, y] = rgb_pred      plt.subplot(nrows, ncols, i + 2)     plt.imshow(image_pred.detach().cpu().numpy())     plt.title(model.get_name())  plt.savefig('plot.png', dpi=300, bbox_inches='tight', transparent=True) plt.show() <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[17]: Copied! <pre>from torchsummary import summary\n</pre> from torchsummary import summary In\u00a0[18]: Copied! <pre>dummy_input = torch.tensor([0.1, 0.2]).to(device)\nmodel = models[0]\n</pre> dummy_input = torch.tensor([0.1, 0.2]).to(device) model = models[0] In\u00a0[19]: Copied! <pre>summary(model, input_size=(2,))\n</pre> summary(model, input_size=(2,)) <pre>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 256]             768\n              ReLU-2                  [-1, 256]               0\n            Linear-3                  [-1, 256]          65,792\n              ReLU-4                  [-1, 256]               0\n            Linear-5                  [-1, 256]          65,792\n              ReLU-6                  [-1, 256]               0\n            Linear-7                  [-1, 256]          65,792\n              ReLU-8                  [-1, 256]               0\n            Linear-9                  [-1, 256]          65,792\n             ReLU-10                  [-1, 256]               0\n           Linear-11                  [-1, 256]          65,792\n             ReLU-12                  [-1, 256]               0\n           Linear-13                  [-1, 256]          65,792\n             ReLU-14                  [-1, 256]               0\n           Linear-15                  [-1, 256]          65,792\n             ReLU-16                  [-1, 256]               0\n           Linear-17                    [-1, 3]             771\n================================================================\nTotal params: 462,083\nTrainable params: 462,083\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.03\nParams size (MB): 1.76\nEstimated Total Size (MB): 1.79\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"experiments/FourierFeatures/#fourier-features-let-networks-learn-high-frequency-functions-in-low-dimensional-domains","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains\u00b6","text":"<ul> <li>Project Page</li> </ul>"},{"location":"experiments/FourierFeatures/#image-regression","title":"Image Regression\u00b6","text":""}]}